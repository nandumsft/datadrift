{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient, Input\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml import load_component\n",
        "\n",
        "# To access files better\n",
        "os.chdir(\"../\")\n",
        "print(os.getcwd())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744179168964
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744179174191
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a handle to workspace\n",
        "ml_client = MLClient.from_config(credential=credential)\n",
        "\n",
        "# Retrieve an already attached Azure Machine Learning Compute.\n",
        "cluster_name = \"cpu-cluster\"\n",
        "# print(ml_client.compute.get(cluster_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744179177871
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744180035041
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment, AmlCompute\n",
        "\n",
        "env_docker_conda = Environment(\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
        "    conda_file=\"./SDK-V2/conda_yamls/env_cli.yml\",\n",
        "    name=\"data-model-drift-env\",\n",
        "    description=\"Environment created from a Docker image plus Conda environment.\",\n",
        ")\n",
        "ml_client.environments.create_or_update(env_docker_conda)\n",
        "\n",
        "env_name = env_docker_conda.name"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744181737449
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import Input\n",
        "\n",
        "parent_dir = os.getcwd()\n",
        "\n",
        "# # Retrieve files from a remote location such as the Blob storage\n",
        "# pred_maintenance_input_remote = Input(\n",
        "#     path=\"azureml://datastores/workspaceblobstore/paths/data_drift/inputs/\", #this path needs to be adjusted to your datastore path\n",
        "#     type= \"uri_folder\"\n",
        "# )\n",
        "\n",
        "# Retrieve files from location location \n",
        "pred_maintenance_input_local =  Input(\n",
        "      type=\"uri_folder\", \n",
        "      path = parent_dir + \"/data/data_raw/predictive_maintenance\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1744179340301
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parent_dir"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744179458945
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to your custom defined components\n",
        "prep_yml = \"/SDK-V2/prep_data.yml\"\n",
        "drift_yml = \"/SDK-V2/data_drift.yml\"\n",
        "drift_db_yml = \"/SDK-V2/data_drift_db.yml\"\n",
        "\n",
        "print(f\"{parent_dir}{prep_yml}\")\n",
        "# 1. Load components\n",
        "prepare_data = load_component(source=f\"{parent_dir}{prep_yml}\")\n",
        "measure_data_drift = load_component(source=f\"{parent_dir}{drift_yml}\")\n",
        "collect_data_drift_values = load_component(source=f\"{parent_dir}{drift_db_yml}\")\n",
        "\n",
        "\n",
        "# 2. Construct pipeline\n",
        "@pipeline()\n",
        "def data_drift_preprocess(pipeline_job_input):\n",
        "    # the parameters come from the respectove .yml file step. E.g. \"input_path\" is under inputs\n",
        "    transform_data = prepare_data(input_path=pipeline_job_input)\n",
        "    # the input for this pipeline is the output of the previous pipeline which is called \"output_path\"\n",
        "    drift_detect = measure_data_drift(\n",
        "        transformed_data_path=transform_data.outputs.output_path,\n",
        "        threshold = 0.01\n",
        "    )\n",
        "    save_drift_db = collect_data_drift_values(\n",
        "        transformed_data_path=transform_data.outputs.output_path,\n",
        "        threshold = 0.01\n",
        "    )\n",
        "    return {\n",
        "        \"pipeline_job_prepped_data\": transform_data.outputs.output_path,\n",
        "        \"pipeline_job_detect_data_drift\": drift_detect.outputs.drift_plot_path,\n",
        "        \"pipeline_job_store_data_drift\": save_drift_db.outputs.drift_db_path,\n",
        "\n",
        "    }\n",
        "\n",
        "# Define the input of your pipeline. In this example we only have one input which is the path to where the input data resides\n",
        "pipeline_job = data_drift_preprocess(pred_maintenance_input_local)\n",
        "\n",
        "\n",
        "# demo how to change pipeline output settings\n",
        "pipeline_job.outputs.pipeline_job_prepped_data.mode = \"upload\" # \"rw_mount\"\n",
        "pipeline_job.outputs.pipeline_job_detect_data_drift.mode = \"upload\" \n",
        "pipeline_job.outputs.pipeline_job_store_data_drift.mode = \"upload\" \n",
        "\n",
        "\n",
        "# set pipeline level compute\n",
        "pipeline_job.settings.default_compute=\"cpu-cluster\"\n",
        "# set pipeline level datastore\n",
        "pipeline_job.settings.default_datastore=\"workspaceblobstore\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744181960075
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submit job to workspace\n",
        "experiment_name = \"data_drift_experiment\"\n",
        "\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline_job, experiment_name=experiment_name\n",
        ")\n",
        "pipeline_job"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1744181972553
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register component"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parent_dir = os.getcwd()\n",
        "\n",
        "# Paths to your custom defined components\n",
        "prep_yml = \"/SDK-V2/prep_data.yml\"\n",
        "prepare_data = load_component(source=f\"{parent_dir}{prep_yml}\")\n",
        "prepare_data_comp = ml_client.components.create_or_update(prepare_data)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744181924188
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-load components to avoid \"reserved name\" error  \n",
        "\n",
        "parent_dir = os.getcwd()\n",
        "\n",
        "# Paths to your custom defined components\n",
        "prep_yml = \"/SDK-V2/prep_data.yml\"\n",
        "drift_yml = \"/SDK-V2/data_drift.yml\"\n",
        "drift_db_yml = \"/SDK-V2/data_drift_db.yml\"\n",
        "\n",
        "# 1. Load components\n",
        "prepare_data = load_component(source=f\"{parent_dir}{prep_yml}\")\n",
        "measure_data_drift = load_component(source=f\"{parent_dir}{drift_yml}\")\n",
        "collect_data_drift_values = load_component(source=f\"{parent_dir}{drift_db_yml}\")\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "\n",
        "# try:\n",
        "#     # try get back the component\n",
        "#     prepare_data_comp = ml_client.components.get(name=\"prepare_drift_data\", version=\"1\")\n",
        "#     measure_data_drift_comp = ml_client.components.get(name=\"measure_data_drift\", version=\"1\")\n",
        "#     collect_data_drift_values_comp = ml_client.components.get(name=\"save_data_drift_values\", version=\"1\")\n",
        "# except:\n",
        "    # if not exists, register component using following code\n",
        "prepare_data_comp = ml_client.components.create_or_update(prepare_data)\n",
        "measure_data_drift_comp = ml_client.components.create_or_update(measure_data_drift)\n",
        "collect_data_drift_values_comp = ml_client.components.create_or_update(collect_data_drift_values)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {prepare_data.name} with Version {prepare_data.version} is registered\",\n",
        "    \"\\n\",\n",
        "    f\"Component {measure_data_drift.name} with Version {measure_data_drift.version} is registered\",\n",
        "    \"\\n\",\n",
        "    f\"Component {collect_data_drift_values.name} with Version {collect_data_drift_values.version} is registered\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1744181959945
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a cron schedule start from current time and fire at minute 0,10 of every hour within the AEST TZ\n",
        "from datetime import datetime\n",
        "from dateutil import tz\n",
        "from azure.ai.ml.constants import TimeZone\n",
        "from azure.ai.ml.entities import (\n",
        "    CronSchedule,\n",
        "    RecurrenceSchedule,\n",
        "    RecurrencePattern,\n",
        "    ScheduleStatus,\n",
        ")\n",
        "\n",
        "schedule_start_time = datetime.now(tz=tz.gettz())\n",
        "cron_schedule = CronSchedule(\n",
        "    expression=\"0,10 * * * *\",\n",
        "    start_time=schedule_start_time,\n",
        "    time_zone=TimeZone.AUS_EASTERN_STANDARD_TIME,\n",
        "    status=ScheduleStatus.ENABLED,\n",
        ")\n",
        "# pipeline_job.schedule = cron_schedule"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submit Jobs via CLI V2\n",
        "\n",
        "**PLEASE EXECUTE THE FOLLOWING COMMANDS IN THE TERMINAL OUTSIDE THIS NOTEBOOK**\n",
        "\n",
        "If you have an error \"the refresh token has expired\", use `az login` to athenticate in the CLI."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!conda activate azureml_py310_sdkv2\n",
        "#!az ml job create --file pipeline.yml"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "index_order": 5,
    "nbconvert_exporter": "python",
    "exclude_from_index": false,
    "pygments_lexer": "ipython3",
    "task": "Classification",
    "deployment": [
      "None"
    ],
    "authors": [
      {
        "name": "ratanase"
      }
    ],
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "name": "python",
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "mimetype": "text/x-python",
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "compute": [
      "AML Compute"
    ],
    "version": "3.6.7",
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "tags": [
      "remote_run",
      "AutomatedML"
    ],
    "datasets": [
      "Creditcard"
    ],
    "file_extension": ".py",
    "category": "tutorial",
    "framework": [
      "None"
    ],
    "friendly_name": "Classification of credit card fraudulent transactions using Automated ML",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}